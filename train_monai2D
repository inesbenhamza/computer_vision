import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from monai.data import DataLoader
from monai.apps import MedNISTDataset
from medNIST import transform_image



transform = transform_image()
training_data = MedNISTDataset(root_dir="./data/MedNIST", section="training", transform=transform)
#validation_data = MedNISTDataset(root_dir="./data/MedNIST", section="validation", transform=transform)

data_length = len(training_data)
train_data = (0.2 * data_length)

data_loader = DataLoader(dataset = train_data, batch_size=32, shuffle=True)


class SimpleCNN(nn.Module):
    def __init__(self, num_classes=6):  
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) 
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #here we are applying 64 filters to the feature map with 32 layers 
        self.fc1 = nn.Linear(64*16*16, 128)  
        self.fc2 = nn.Linear(128, num_classes)
        self.pool = nn.MaxPool2d(2, 2)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 16 * 16)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


model = SimpleCNN(num_classes=6)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)


for epoch in range(5):
    model.train()
    running_loss = 0.0

    for batch in data_loader:
        images = batch["image"]
        labels = batch["label"]

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {running_loss:.4f}")
